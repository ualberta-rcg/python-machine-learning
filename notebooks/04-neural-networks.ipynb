{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-harvey",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Hopefully you've watched the three videos by [Grant Sanderson](https://twitter.com/3blue1brown) (a.k.a. [3blue1brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)).\n",
    "\n",
    "* [But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk) (19:13)\n",
    "* [Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w) (21:00)\n",
    "* [What is back propagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U) (13:53)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-plate",
   "metadata": {},
   "source": [
    "## A very brief recap from the homework\n",
    "\n",
    "**Neurons**:\n",
    "\n",
    "* Hold a value\n",
    "* This value is related to the values of neurons on previous layers via:\n",
    "    * **weights**\n",
    "    * **bias**\n",
    "    * **activation function**\n",
    "* Some jargon: weights and biases are called **parameters** of the model (they are estimated from data automatically). The other options about the model are called **hyperparameters**.\n",
    "\n",
    "**Neural network structure**:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1d/Neural_network_example.png\"  style=\"width:200px;\">\n",
    "\n",
    "* Input layer\n",
    "* one or more hidden layers (this is where the term \"deep\" comes from)\n",
    "* an output layer\n",
    "\n",
    "**Learning**:\n",
    "\n",
    "* Minimizing a **loss function** (or **cost function**) through back propagation\n",
    "  * Loss is often **Mean Squared Error** (**MSE**) between the labels and the predicted labels\n",
    "* An **optimizer** helps find the best possible parameters\n",
    "  * Data is fed to the model with the current weights and biases, and the optimizer instructs how to adjust the weights and biases, and the process is iterated.\n",
    "  * This can be **gradient descent**, which is a slow process.\n",
    "  * The choice of optimizer might mean the difference between a model that is trained in minutes vs days.\n",
    "  * each time the entire set of data is fed to the algorighm, it is called an **epoch**\n",
    "  * some times the adjustment process can be sped up by feeding in the data in smaller **batches** (usually randomly selected) and adjusting the weights more frequently.\n",
    "    * an example of this strategy is **stochastic gradient descent**\n",
    "    * a modern extention to stochastic gradient descent optimizer is the **Adam** optimizer, which is now very commonly used. The math is pretty heavy, but you can read about some of the details here: [Gentle Introduction to the Adam Optimization Algorithm for Deep Learning](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "\n",
    "<img src=\"assets/silly-tshirt.png\"  style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b1d52",
   "metadata": {},
   "source": [
    "Now that we have some concepts defined, let's play around with a neural network before touching any code:\n",
    "\n",
    "https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22356365",
   "metadata": {},
   "source": [
    "## Download Data and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494fb79",
   "metadata": {
    "tags": [
     "download"
    ]
   },
   "outputs": [],
   "source": [
    "# Download data and solutions\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_data(path, branch='main'):\n",
    "    base_url = 'https://raw.githubusercontent.com/ualberta-rcg/python-machine-learning'\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    if not os.path.exists('data/titanic'):\n",
    "        os.mkdir('data/titanic')\n",
    "    if not os.path.exists('data/numbers'):\n",
    "        os.mkdir('data/numbers')\n",
    "    url = '{}/{}/notebooks/{}'.format(base_url, branch, path)\n",
    "    output_file = path\n",
    "    urllib.request.urlretrieve(url, output_file)\n",
    "    print(\"Downloaded \" + path)\n",
    "    \n",
    "download_data('data/titanic/train.csv')\n",
    "download_data('data/numbers/cwant_1.png')\n",
    "download_data('data/numbers/cwant_3.png')\n",
    "download_data('data/numbers/cwant_5.png')\n",
    "download_data('data/numbers/cwant_8.png')\n",
    "download_data('data/numbers/cwant_thick_1.png')\n",
    "download_data('data/numbers/cwant_thick_3.png')\n",
    "download_data('data/numbers/cwant_thick_4.png')\n",
    "download_data('data/numbers/cwant_thick_5.png')\n",
    "download_data('data/numbers/cwant_thick_6.png')\n",
    "download_data('data/numbers/cwant_thick_9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b5ba5",
   "metadata": {},
   "source": [
    "Like other package we have seen, Keras has a submodule of sample datasets. The **MNIST** dataset of handwritten numbers is included, which we can load as both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4c7f",
   "metadata": {},
   "source": [
    "We can see how many samples are in the **training** features data, and the shape of each sample ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf03f8",
   "metadata": {},
   "source": [
    "Same for the **test** data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f1363c",
   "metadata": {},
   "source": [
    "We can look at an individual sample in the training data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[31] # 32-nd record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d50e61",
   "metadata": {},
   "source": [
    "But it probably makes more sense to convert this data into an image and render it. The `PIL` module makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf364b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "PIL.Image.fromarray(x_train[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d0636",
   "metadata": {},
   "source": [
    "We can then check the label to see that the image corresponds to the number we think it is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640511be",
   "metadata": {},
   "source": [
    "We will now transform the feature data to convert each 28 * 28 image to a 784 entry array through the `reshape` method from `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c568ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(60000, 784)\n",
    "X_test = x_test.reshape(10000, 784)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c93bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of 28x28 inputs\n",
    "print(x_train[128][14][13])\n",
    "\n",
    "# Array of 784 inputs\n",
    "# basically each of the 28 rows is shoved at the end of the previous\n",
    "print(X_train[128][14*28+13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a1211",
   "metadata": {},
   "source": [
    "And we can convert the numbers in the label data to categorial data (basically one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import keras.utils as ku\n",
    "    # API change ... is the function in here?\n",
    "    type(ku.to_categorical)\n",
    "except:\n",
    "    import keras.utils.np_utils as ku\n",
    "\n",
    "Y_train = ku.to_categorical(y_train, 10)\n",
    "Y_test = ku.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca69c1",
   "metadata": {},
   "source": [
    "The original y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfe35b",
   "metadata": {},
   "source": [
    "The new ones look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5981ae3",
   "metadata": {},
   "source": [
    "Getting the previous value is essentially running `argmax` (the index of the largest value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[26].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-guide",
   "metadata": {},
   "source": [
    "## Sequential model\n",
    "\n",
    "Sequential groups a linear stack of layers. The code below:\n",
    "\n",
    "* Specifies the input layer as having 784 items\n",
    "* Has an intermediate layer with 128 nodes\n",
    "* Has an output layer of 10 nodes\n",
    "\n",
    "Eash layer has a `sigmoid` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-landing",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbabf49",
   "metadata": {},
   "source": [
    "Compiling prepares the model for training.\n",
    "\n",
    "The optimizer chosen here is `sgd` (Stochastic Gradient Descent).\n",
    "\n",
    "The loss/cost function we will use is `mean_squared_error`.\n",
    "\n",
    "The accuracy is reported during training for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-motor",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Gradient Descent is a slow process, so one speed up is to send the data to the algorithms in random batches until all of the data is read (Stochastic Gradient Descent). Each time all of the data is fed into the model for training, it's called an **epoch**.\n",
    "\n",
    "An epoch can be split into **minibatch** (or just **batch**), between which the model's parameters are updated.\n",
    "\n",
    "So the number of epochs you train is how many times the model will see each training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    batch_size=100,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61222964",
   "metadata": {},
   "source": [
    "We can now check out the accuracy of our model on our unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd9031",
   "metadata": {},
   "source": [
    "What's up with that `history` variable that's output from training? It provides some information about the loss and accuracy for each epoch.\n",
    "\n",
    "We can use this to plot the loss and accuracy over the epochs for this training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'accuracy']].plot();\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy/Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8241e3",
   "metadata": {},
   "source": [
    "**Now run the training and evaluation cells again.** (Training continues where we left off, and we can continue training the same model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b9165",
   "metadata": {},
   "source": [
    "## Exercise: the ultimate test\n",
    "\n",
    "Now the ultimate test: can this model correctly detect **your** hand-drawn numbers?\n",
    "\n",
    "You might want to try drawing your own number here:\n",
    "\n",
    "https://drawisland.com/?w=200&h=200\n",
    "\n",
    "Rules:\n",
    "* Draw a digit with a black pen on a white background (default)\n",
    "* Perhaps bump up the pen size\n",
    "* Click the **Download** button to save a `png` file to your computer (hint: put the digit you drew as part of the filename).\n",
    "* Put the image (or upload to Colab) in the subdirectory `data/numbers` of your current workbook directory. There should be some `png` files of numbers I drew already in there.\n",
    "\n",
    "To figure out the current notebook directory, uncomment one of the lines with the exclamation mark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778bed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux/Mac/Colab\n",
    "# !pwd\n",
    "\n",
    "# Windows\n",
    "# !dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39a83a",
   "metadata": {},
   "source": [
    "We can write a function that loads/displays/transforms/predicts an image file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b542bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "import numpy as np\n",
    "\n",
    "def image_predict(model, filename):\n",
    "    # Load and resize to 28x28\n",
    "    image = PIL.Image.open(filename).convert('L').resize((28,28))\n",
    "    # Switch black and white\n",
    "    image = PIL.ImageOps.invert(image)\n",
    "    # Display\n",
    "    print(\"Filename: {}\".format(filename))\n",
    "    print(\"Image:\")\n",
    "    display(image)\n",
    "    # Convert to numpy array and reshape as 784 length vector\n",
    "    image_array = np.array(image)[:,:].reshape(784)\n",
    "    # Predict!\n",
    "    prediction = model.predict(np.array([image_array])).argmax()\n",
    "    print(\"Prediction: {}\\n\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81c098",
   "metadata": {},
   "source": [
    "We can now test it out on your file (replace `cwant_8.png` with your filename):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('data/numbers/*.png'):\n",
    "image_predict(model, 'data/numbers/cwant_8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e58ad1",
   "metadata": {},
   "source": [
    "Did the model predict the correct number?\n",
    "\n",
    "We use the `glob` module to predict all of the numbers in the `data/numbers` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bf04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for filename in glob.glob('data/numbers/*.png'):\n",
    "    image_predict(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29871928",
   "metadata": {},
   "source": [
    "How did you the model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185de8a",
   "metadata": {},
   "source": [
    "## How about adding another layer?\n",
    "\n",
    "After adding another layer, we'll have more than three layers (including input and output), so our network is considered to be **deep** (and we are doing **deep learning**). In general, the deeper the network, the more complex learning it can do (at the cost of having to optimize many more parameters, which takes longer).\n",
    "\n",
    "We will also add use a feature that allows for the test data to be validated on each epoch (the `validation_data` argument of the `fit` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid',\n",
    "                   name = 'hidden'))\n",
    "model.add(kl.Dense(128, activation = 'sigmoid', name = 'hidden2'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We will take the default batch size (32)\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "# Note, `verbose=1` is the default, so omitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745bfd5",
   "metadata": {},
   "source": [
    "And try the test on our hand-drawn characters again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cd3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for filename in glob.glob('data/numbers/*.png'):\n",
    "    image_predict(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11526e5c",
   "metadata": {},
   "source": [
    "## GPUs ...\n",
    "\n",
    "Record the time to train the model above.\n",
    "\n",
    "We might consider running on a GPU if your computer has one (and if tensorflow and the libraries on your computer are set up to use one). We can check ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb470f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7d3d3",
   "metadata": {},
   "source": [
    "If you are running in Colab, you might want to change your run time type and choose 'GPU'.\n",
    "(After doing that, run the check above again)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93284fd9",
   "metadata": {},
   "source": [
    "If you did change run time, your notebook will be running on a different computer.\n",
    "\n",
    "The next cell will get you caught up ... (but also run the [download data cell](#Download-Data-and-Solutions) at the top of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "try:\n",
    "    import keras.utils as ku\n",
    "    type(ku.to_categorical)\n",
    "except:\n",
    "    import keras.utils.np_utils as ku\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "import numpy as np\n",
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def image_predict(model, filename):\n",
    "    # Load and resize to 8x8\n",
    "    image = PIL.Image.open(filename).resize( (28,28) ).convert( 'L' )\n",
    "    # Switch black and white\n",
    "    image = PIL.ImageOps.invert(image)\n",
    "    # Display\n",
    "    print(\"Filename: {}\".format(filename))\n",
    "    print(\"Image:\")\n",
    "    display(image)\n",
    "    # Convert to numpy array and reshape as 784 length vector\n",
    "    image_array = np.array(image)[:,:].reshape(784)\n",
    "    # Predict!\n",
    "    prediction = model.predict(np.array([image_array])).argmax()\n",
    "    print(\"Prediction: {}\\n\".format(prediction))\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = x_train.reshape(60000, 784)\n",
    "X_test = x_test.reshape(10000, 784)\n",
    "Y_train = ku.to_categorical(y_train, 10)\n",
    "Y_test = ku.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046ee1b",
   "metadata": {},
   "source": [
    "Now run the model again and watch the timing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(128, activation = 'sigmoid', name = 'hidden2'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "# We will take the default batch size (32)\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c6308",
   "metadata": {},
   "source": [
    "Did you get a performance boost?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd5c10",
   "metadata": {},
   "source": [
    "## How about just a wider layer?\n",
    "\n",
    "Another way to have a network learn more complex patterns is with wider layers. Notice the reduced number of epochs, and how quick the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.Dense(1024, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=7,\n",
    "                    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for filename in glob.glob('data/numbers/*.png'):\n",
    "    image_predict(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af22881",
   "metadata": {},
   "source": [
    "## How about a different activation function?\n",
    "\n",
    "![Popular activation functions, source: https://www.researchgate.net/publication/335845675_Reconstruction_of_porous_media_from_extremely_limited_information_using_conditional_generative_adversarial_networks](assets/common-activation.png)\n",
    "\n",
    "Our current activation is a sigmoid.\n",
    "\n",
    "There is another very popular activation function called \"The Rectified Linear Unit\" (ReLU) that is used in machine learning.\n",
    "\n",
    "* ReLU has the advantage that it makes the math easier\n",
    "* Sigmoid sometimes has a problem where the gradient can vanish (so gradient descent doesn't really step anywhere). ReLU has constant gradient in activation zone.\n",
    "* ReLU has it's own problems: it can \"blow up\" (see that it's not bounded above).\n",
    "\n",
    "Lets set up our model again to use ReLu for one of the hidden layers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(128, activation = 'relu', name = 'hidden2'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc77f2",
   "metadata": {},
   "source": [
    "Again, we can plot our loss/accuracy, this time with the validation data too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d621e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, history_df.columns].plot();\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12655619",
   "metadata": {},
   "source": [
    "## Different optimizer (adam)\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is considered the state of the art of optimizers (currently).\n",
    "\n",
    "A full description of how it works is beyond the scope of this course, but you can check out some comparisons of optimizers here.\n",
    "\n",
    "https://medium.com/swlh/strengths-and-weaknesses-of-optimization-algorithms-used-for-machine-learning-58926b1d69dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(128, activation = 'relu', name = 'hidden2'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98487fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4eb644",
   "metadata": {},
   "source": [
    "Do you notice anything different with this loss/accuracy plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, history_df.columns].plot();\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d99161",
   "metadata": {},
   "source": [
    "... and another look at predictions based on our self-generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob('data/numbers/*.png'):\n",
    "    image_predict(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61fc860",
   "metadata": {},
   "source": [
    "## A third ReLu layer ...\n",
    "\n",
    "Why not? (Well, because it will take longer to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e02d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(128, activation = 'relu', name = 'hidden2'))\n",
    "model.add(kl.Dense(128, activation = 'relu', name = 'hidden3'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob('data/numbers/*.png'):\n",
    "    image_predict(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb739e",
   "metadata": {},
   "source": [
    "## Stopping early ...\n",
    "\n",
    "That last training example seemed to converge pretty quickly, so the additional benefits of the extra epochs may not have been worth it. We can configure our training to have a `callback` that checks whether a certain condition has occured, and stops early if directed to do so.\n",
    "\n",
    "In this case, we exit early if we haven't had sufficient change in the validation loss in a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=5, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 784, activation = 'sigmoid', name = 'hidden'))\n",
    "model.add(kl.Dense(128, activation = 'relu', name = 'hidden2'))\n",
    "model.add(kl.Dense(128, activation = 'relu', name = 'hidden3'))\n",
    "model.add(kl.Dense(10, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=25,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac4bc6",
   "metadata": {},
   "source": [
    "## Saving models\n",
    "\n",
    "So you've spent a lot of time training a model... now what? If we want to use the model in the future, do we have to retrain your model again?\n",
    "\n",
    "No. What you probably want to do is save your trained model for use elsewhere.\n",
    "\n",
    "A potential workflow:\n",
    "\n",
    "* Train your model on an HPC cluster\n",
    "* Dump and download your model\n",
    "* Use your model to predict elsewhere\n",
    "\n",
    "Converting your in-memory data into a form that can be written to disk (and read again later) is called **serialization**. For generic use cases, Python comes with a popular package for serializing variables called **`pickle`**.\n",
    "\n",
    "The Keras documentation has a section on how to serialize and save your trained models, using some methods that are defined for the model objects.\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/save_and_serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49178feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51190a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.evaluate(X_test,  Y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for filename in glob.glob('data/numbers/*.png'):\n",
    "    image_predict(loaded_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acf602",
   "metadata": {},
   "source": [
    "## Titanic revisited\n",
    "\n",
    "Let's look at how well neural networks do on our original Titanic classification problem.\n",
    "\n",
    "Our pipeline starts our identical to what we've already seen ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Cherry picked seed!\n",
    "np.random.seed(1337)\n",
    "# This one does some strange stuff\n",
    "#np.random.seed(1)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "\n",
    "# Choose features and lables\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(train_df[features], drop_first=True)\n",
    "# Note: some versions of tensorflow might instead need:\n",
    "# X = pd.get_dummies(train_df[features], drop_first=True).values.astype(np.float32)\n",
    "\n",
    "\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbdfc9",
   "metadata": {},
   "source": [
    "Here is a basic network that performs about as well as the previous Decision Tree/Random Forest models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75856209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 4, activation = 'sigmoid',\n",
    "                   name = 'hidden'))\n",
    "model.add(kl.Dense(1, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee234c",
   "metadata": {},
   "source": [
    "We will use early stopping again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232df77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c786ef",
   "metadata": {},
   "source": [
    "**Do you notice a difference between the accuracy and the validation/test accuracy? What do you suppose this means?**\n",
    "\n",
    "We can now use the model to make predictions on the test data (and look at the first few values) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict on unseen test data\n",
    "# Some tricks needed to predict as integers\n",
    "predictions = model.predict(X_test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405c0df",
   "metadata": {},
   "source": [
    "Notice that we have solved a regression problem, not a classification problem!\n",
    "\n",
    "We can convert the predicted values for the `Survived` column by selecting a cut off..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using 0.5 for a cutoff, but we may want to\n",
    "# use some other value to prevent false positives/negatives\n",
    "def cut_off(x):\n",
    "    if x < 0.5: return 0\n",
    "    return 1\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions = [cut_off(x) for x in predictions]\n",
    "predictions\n",
    "\n",
    "# Evaluate how well the model did\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))\n",
    "\n",
    "# I need this or my head will explode ...\n",
    "confusion_matrix(y_test, predictions)\n",
    "# [TP FP]\n",
    "# [FN TN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab9719",
   "metadata": {},
   "source": [
    "**Change the cut off above. How do the precision and recall values change?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa39392",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "When there is a gap between the training and testing accuracy/loss, this is evidence that overfitting is going on (the model performs better on the training data that it does on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, history_df.columns].plot();\n",
    "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d022e",
   "metadata": {},
   "source": [
    "Two common ways to handle overfitting are through `regularization` and `dropping out`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f20e7e",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a method we can use to tackle overfitting.\n",
    "\n",
    "To quote the SciNet neural networks workshop:\n",
    "\n",
    "\"Regularization is an ad hoc technique by which parameters in a model are penalized to prevent\n",
    "individual parameters from becoming excessively important to the fit.\"\n",
    "\n",
    "This technique involves a modification to the cost function our training uses to treat (the extent to which high parameters are penalized is controlled by a parameter lambda ($\\lambda$). (Note that we can't name the parameter `lambda` below, because `lambda` is a reserved keywork in python, so we call in `lam`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d90690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.regularizers as kr\n",
    "\n",
    "lam = 0.001\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 4, activation = 'sigmoid',\n",
    "                   name = 'hidden',\n",
    "                   kernel_regularizer = kr.l2(lam)))\n",
    "model.add(kl.Dense(1, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90973019",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0b5e",
   "metadata": {},
   "source": [
    "## Dropping out ...\n",
    "\n",
    "Again, to quote the SciNet neural networks workshop:\n",
    "\n",
    "\"The principle is simple: randomly ”drop out” neurons from the network during each batch\n",
    "of the stochastic gradient descent. Like regularization, this results in the network not putting too much importance on any given weight, since the weights keep randomly disappearing from the network.\n",
    "It can be thought of as averaging over several different-but-similar neural networks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.Dense(128, input_dim = 4, activation = 'sigmoid',\n",
    "                   name = 'hidden'))\n",
    "# apply 30% dropout to the next layer\n",
    "model.add(kl.Dropout(0.3))\n",
    "model.add(kl.Dense(1, name = 'output', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "              loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a265266",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf4c8a",
   "metadata": {},
   "source": [
    "## Some references\n",
    "\n",
    "* The **SciNet workshop** on neural networks:\n",
    "\n",
    "  https://support.scinet.utoronto.ca/education/go.php/451/index.php/ib/1//p_course/451\n",
    "  \n",
    "  This course goes a lot deeper into the mathematics of neural networks.\n",
    "\n",
    "* The **Kaggle course** on neural networks\n",
    "\n",
    "  https://www.kaggle.com/learn/intro-to-deep-learning\n",
    "  \n",
    "  A nice interactive approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee27586",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "\n",
    "* Convolutional Neural Networks\n",
    "  * https://adamharley.com/nn_vis/cnn/2d.html\n",
    "* Transfer learning\n",
    "  * Using pre-trained neural networks as an initial base for more specific training\n",
    "* Free book!\n",
    "  * http://neuralnetworksanddeeplearning.com/\n",
    "* Kaggle courses\n",
    "  * https://www.kaggle.com/learn\n",
    "  * Do tutorials\n",
    "  * Each tutorial has a challenge notebooks to complete to get credit\n",
    "  * At the end of the course you get a certificate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
