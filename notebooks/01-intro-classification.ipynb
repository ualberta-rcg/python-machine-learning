{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Learn techniques in data science\n",
    "* Practice programming in Python and using data frames\n",
    "* Learn the jargon and understand what machine learning pipelines look like\n",
    "* Set up a classification model\n",
    "* Set up a regression model\n",
    "* Set up an unsupervised (clustering) model\n",
    "* Explore some basic neural networks\n",
    "\n",
    "---\n",
    "\n",
    "**Approach to learning**: I would like to take an approach where we are playing with data as early as possible. As such, we will be talking about concepts as we encounter them in the data.\n",
    "\n",
    "---\n",
    "\n",
    "**Assumptions**: you have at least 12 hours of Python training, including some exposure to Pandas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "\n",
    "Loosly, machine learning can be defined as:\n",
    "\n",
    "```Algorithms that allow a computer to predict patterns in unseen data based on learning done on data that has previously been seen```\n",
    "\n",
    "Typically, the more data you allow the computer to learn with, the better job it will do.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use machine learning?\n",
    "\n",
    "A non-exhaustive list of applications:\n",
    "\n",
    "* Fraud detection\n",
    "* Spam detection\n",
    "* Credit risk\n",
    "* Voice recognition\n",
    "* Image recognition\n",
    "* Recommendations (search engines)\n",
    "* Finding patterns in the stock market\n",
    "* Housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional modeling vs machine learning\n",
    "\n",
    "Usually in science we work with **white box model**, like a set of equations, and we fully understand how our models work. We can see the process that turns inputs into outputs.\n",
    "\n",
    "With machine learning, we let the machine figure out the details. While we understand the algorithms that allow the computer to learn, we don't always understand the insights or details about the specific insights the algorithms learn about the data (**black box model**).\n",
    "\n",
    "This aspect is sometimes used as an argument against the practice of machine learning.\n",
    "\n",
    "Here is a thought provoking [blog entry about this from Rich Sutton](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) (pioneer of reinforcement learning, U of A faculty).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process\n",
    "\n",
    "\n",
    "* **Define the problem**\n",
    " * This can be difficult. What are we trying to achieve? What are we trying to predict?\n",
    "* **Get the data**\n",
    " * This is often connected with the problem definition step, because knowing about the data helps clarify what we can do with it\n",
    "* **Prepare data** \n",
    " * Exploratory data analysis and visualization\n",
    " * Cleaning data\n",
    " * Often the most tedious and time consuming step\n",
    "* **Select Algorithm**\n",
    " * Setting up one or more machine learning pipelines \n",
    "* **Train the model**\n",
    " * Feed the algorithm data.\n",
    "* **Test the mode**\n",
    "  * Maybe we need to go back and select a different algorithm to work with?\n",
    "* **Select the best model** \n",
    " * The definition of \"best\" depends on the type of problem, the type of data, and our goals\n",
    "* **Predict**\n",
    "  * Use the model to make predictions based on unseen data\n",
    "* $$$ (?)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data science tools in Python we will be using\n",
    "\n",
    "* Data analysis and cleaning/transforming: **pandas**\n",
    "* Visualization: matplotlib (possibly **seaborn** and **plotly**)\n",
    "* Scientific computing/number crunching: **numpy**\n",
    "* Machine learning algorithms: **Scikit-learn**\n",
    "* Neural networks: **keras** (using **tensorflow** as a backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make sure that we have the tools available to us ...\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... if not, may need to uncomment one or more of:\n",
    "# (Replace pip with conda where applicable)\n",
    "\n",
    "# !pip install tensorflow\n",
    "# !pip install keras\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data for this notebook\n",
    "\n",
    "We can download the data if we don't have it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and solutions\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_data(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    if not os.path.exists('data/titanic'):\n",
    "        os.mkdir('data/titanic')\n",
    "    if not os.path.exists('solutions'):\n",
    "        os.mkdir('solutions')\n",
    "    url = 'https://raw.githubusercontent.com/ualberta-rcg/python-machine-learning/main/notebooks/' + path\n",
    "    output_file = path\n",
    "    urllib.request.urlretrieve(url, output_file)\n",
    "    print(\"Downloaded \" + path)\n",
    "\n",
    "def show_solution(file):\n",
    "    fp = open('solutions/{}'.format(file), 'r')\n",
    "    print(fp.read())\n",
    "\n",
    "download_data('data/titanic/train.csv')\n",
    "download_data('solutions/titanic-passenger-class.py')\n",
    "download_data('solutions/titanic-random-forest-pipeline.py')\n",
    "download_data('solutions/titanic-age-dropna.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Machine Learning tools\n",
    "\n",
    "Four packages that are available to us for free:\n",
    "\n",
    "* Scikit-learn\n",
    " * Easy to understand\n",
    " * Great for learning\n",
    " * Consistent interface\n",
    "* Tensorflow\n",
    " * From Google\n",
    " * Takes advantage of GPUs\n",
    "* Pytorch\n",
    " * From Facebook\n",
    " * Takes advantage of GPUs\n",
    "* Keras\n",
    " * Built on top of Tensorflow\n",
    " * Easier to understand and use\n",
    "\n",
    "Some comparisons:\n",
    "https://towardsdatascience.com/scikit-learn-tensorflow-pytorch-keras-but-where-to-begin-9b499e2547d0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A problem: The Titanic Kaggle challenge\n",
    "\n",
    "We will learn some of the concepts and jargon of machine learning by walking through an example.\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/) (owned by Google) is a machine learning competition website.\n",
    "\n",
    "Competitions are either for fun, for money, or might lead to a job offer.\n",
    "\n",
    "The introductory competition involves the sinking of the [Titanic](https://www.kaggle.com/c/titanic):\n",
    "\n",
    "> The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    ">\n",
    "> On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "> \n",
    "> While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    ">\n",
    "> In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n",
    "\n",
    "In the competition, we are given 891 rows of data (one row per passenger) in the file `train.csv` where we know whether the passenger survived. As the name indicates, we will use this data to train a machine learning model.\n",
    "\n",
    "For the competition there is also a file called `test.csv` that includes information on 418 other passengers, but missing from this data is whether the passenger survived or not. To enter the competition, we make predictions on this file and submit it to the competition.\n",
    "\n",
    "When I was first learning data science, I did two submissions to the Titanic competition: one which guessed 71% correctly, the other guessed 76% correctly.\n",
    "\n",
    "We'll explore this problem to learn some of the concepts related to machine learning, and build a model that at least beats my earlier attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An initial look ...\n",
    "\n",
    "We'll load the data file in and get a sense for the data it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the first 10 records ...\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any good candidates for predictors of survival?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "\n",
    "In short, inputs to a machine learning model are called **features** (or **attributes**), and the output predicted is called a **label** (or a **target**).\n",
    "\n",
    "The **label** that we are trying to predict in the Titanic challenge is clearly the information in the **Survived** data column.\n",
    "\n",
    "We have 11 potential **features** we can use to predict this **label**.\n",
    "\n",
    "* A feature can be binary, nominal, or numerical.\n",
    "* We want to choose features that have predictive power.\n",
    "* We want to choose features that are as independent as possible\n",
    "    * E.g., if `weight_in_pounds` is a predictive feature in a model, don't also choose `weight_in_kilograms`\n",
    "* Note that we can also design our own features from the data provided. (E.g., if we are predicting stock prices and we are given the previous opening and closing prices of a stock, maybe the difference would be a good predictor?). The process of designing and choosing features is called **feature engineering**.\n",
    "\n",
    "A row of features (a single record in the input) is often called a **feature vector**.\n",
    "\n",
    "A **dataset** often refers to a collection of feature vectors (either labeled or unlabeled)\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: what could be possible features for a model that predicts housing prices?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Learning\n",
    "\n",
    "### Supervised\n",
    "* All instances in training data are **labeled**\n",
    "* **Classification** - predicting nominal label\n",
    "  * We are looking to build models that separate data into distinct classes\n",
    "  * Algorithms:\n",
    "    * Decision Trees\n",
    "    * Random Forest\n",
    "    * Support Vector Machine\n",
    "  * E.g.,\n",
    "    * Did the person survive the titanic? (True/False)\n",
    "    * What species of plant is this?\n",
    "* **Regression** - predicting numerical label\n",
    "  * Based on previous data, predict a continuous numerical quantity\n",
    "  * Algorithms\n",
    "    * Linear regression\n",
    "    * Polynomial regression\n",
    "  * E.g.,\n",
    "    * Predict the high temperature for tomorrow\n",
    "    * Predict the closing price of a stock tomorrow\n",
    "\n",
    "### Unsupervised\n",
    "* There are **no labels** for the instances\n",
    "* We are trying to find hidden meaning in data without additional guidance\n",
    "* E.g., Find ten categories that a collection of emails fall into (clustering)\n",
    "* Algorithms:\n",
    "  * KMeans\n",
    "\n",
    "### Reinforcement Learning\n",
    "* Algorithms learn how to make actions on data points based on environment responses\n",
    "* It’s impossible to get label without making an action\n",
    "* Check this out, a DeepMind model learning to play Atari Breakout: https://www.youtube.com/watch?v=V1eYniJ0Rnk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is poking around data (e.g., looking at descriptive statistics or plots) to gather insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The `value_counts` method for a series is useful for exploring the data. It in essense gives us the frequency table from a column in the data.\n",
    "\n",
    "Lets find out the survival rate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_df['Survived'].value_counts()\n",
    "percent = round(100 * counts[1] / len(train_df))\n",
    "print('Survival rate: {}%'.format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the influence of various features on survival rate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_counts = train_df['Sex'].value_counts()\n",
    "sex_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the series returned from `value_counts` is organized\n",
    "\n",
    "print(sex_counts.keys())\n",
    "print(sex_counts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data for the males alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df = train_df[train_df['Sex'] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now the survival rate for males ...\n",
    "\n",
    "counts = male_df['Survived'].value_counts()\n",
    "percent = round(100 * counts[1] / len(male_df), 2)\n",
    "print('Survival rate: {}%'.format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you will have recognized that you have done an almost identical set of operations on two different dataframes ... this sounds like a good time to create a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_report(df):\n",
    "    if len(df) == 0:\n",
    "        print('Empty data')\n",
    "        return\n",
    "    counts = df['Survived'].value_counts()\n",
    "    print('             Lived: {}'.format(counts[1]))\n",
    "    print('              Died: {}'.format(counts[0]))\n",
    "    print('Chance of survival: {}%'.format(round(100 * counts[1] / len(df), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_report(male_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new function makes exploring the data for the female passengers easy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_report(train_df[train_df['Sex'] == 'female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: is gender a good predictor of survival?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Using our `survival_report` function above, explore the survival rates for the different values of Passenger class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "show_solution('titanic-passenger-class.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting some features\n",
    "\n",
    "Clearly `Sex` and `Pclass` have information that would be valuable to a model that would predict survival on the Titanic, so we will include them in our collection of features.\n",
    "\n",
    "We will add a couple of other features:\n",
    "* `SibSp` (number of siblings/spouses also aboard)\n",
    "* `Parch` (number of parents/children also aboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset with just these features is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Most machine learning algorithms only work on numerical features, not with categorical strings like 'male' and 'female'.\n",
    "\n",
    "**One-hot** encoding is the process of converting categorical variables into a binary representation.\n",
    "\n",
    "There is a method of a pandas `Dataframe` called [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) that does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(train_df[features])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_dummies` method leaves the other columns (which already have numerical information) untouched.\n",
    "\n",
    "(Question: while passenger class is expressed as a number, should it perhaps also be treated as nominal?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sex_female` column doesn't provide any additional predictive information when combined with the `Sex_male` column (the information it contains is entirely redundant), so we can ask `get_dummies` to leave it out with the `drop_first` keyword option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(train_df[features], drop_first=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`X` is now a dataframe with our features. We now define our labels as `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['Survived']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The `X` is uppercase becase it is a set of vectors, the `y` is lowercase because it is a set of individual values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training and test data\n",
    "\n",
    "We would like to now (randomly) split our data (both the features, and the labels) into two sets:\n",
    "\n",
    "* Data we will **train** a machine learning model on\n",
    "* Data we will use to **test** our machine learning model with to test the model's performance. This data will be previously **unseen** by the training process.\n",
    "\n",
    "Arbitrarily, we can decide that we would like **two thirds of the data to train on**, reserving the remaining one third of the data for evaluating the model (we make predictions with the test data, then compare these predictions with the real answers).\n",
    "\n",
    "Scikit-learn has a function called [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) that makes this work easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the names indicate:\n",
    "* `X_train` are the features that are used for training (2/3 of the `X` values)\n",
    "* `y_train` are the labels that are used for training (2/3 of the `y` values)\n",
    "* `X_test` are the features used to make predictions to test the model (1/3 of the `X` values)\n",
    "* `y_test` are the labels that are used to compare the predictions with reality (1/3 of the `y` values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Our first machine learning model: Decision Tree\n",
    "\n",
    "The model we are going to initially try is a decision tree.\n",
    "\n",
    "![](https://raw.githubusercontent.com/ualberta-rcg/python-machine-learning/main/notebooks/assets/youdroppedfood.jpg)\n",
    "\n",
    "(Image: Audrey Fukman and Andy Wright on SFoodie, via Serious Eats)\n",
    "\n",
    "A number of questions are asked about the data, and decisions are made based on the answers. These answers are refined or changed based on additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "The [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) looks at your data and figures out what questions to ask automatically.\n",
    "\n",
    "We can specify an option for `max_depth` (basically, how deep do we want the questions to go)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `fit`\n",
    "\n",
    "Most models (all?) from Scikit-learn have a `fit` method that trains the model based on your features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit doesn't modify the model in place\n",
    "# (returns a trained model)\n",
    "\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `predict`\n",
    "\n",
    "Most models (all?) from Scikit-learn have a `predict` method that allows a trained model to make predictions when given unlabeled features.\n",
    "\n",
    "Here we use our **test data** (data that the model wasn't trained on) as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first ten predicted labels for the test data\")\n",
    "print(list(predictions[:10]))\n",
    "print('The ten actual labels')\n",
    "print(list(y_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what did the decision tree do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to install graphviz ....\n",
    "# Note for conda, you may have to install both graphviz and python-graphviz\n",
    "# !pip install graphviz\n",
    "# !conda install graphviz python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "dot_data = export_graphviz(model,\n",
    "                           feature_names=X.columns,\n",
    "                           class_names=['Died', 'Survived'],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True,\n",
    "                           out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the quality of predictions in classification models\n",
    "\n",
    "Ah, statistics and jargon! Let's look at something called a [**confusion matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) that compares our predicted values to the actual values in `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix reports the number of true positive, true negative, false positive, and false negative predictions in a table:\n",
    "\n",
    "|          | Predicted 0         | Predicted 1         |\n",
    "|----------|---------------------|---------------------|\n",
    "| **Actual 0** | True negative (TN)  | False positive (FP) |\n",
    "| **Actual 1** | False negative (FN) | True positive (TP)  |\n",
    "\n",
    "Entries on the main diagonal report correct predictions, entries on the other diagonal report incorrect predictions.\n",
    "\n",
    "Here we have the following interpretations for our predictions:\n",
    "\n",
    "* Correct predictions\n",
    "  * **True negative**: person was predicted to die and actually died\n",
    "  * **True positive**: person was predicted to survive and actually survived\n",
    "* Incorrect predictions\n",
    "  * **False negative**: person was predicted to die, but actually survived\n",
    "  * **False positive**: person was predicted to survive but actually died"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** is the proportion of predictions that are correct (The proportion of true positives and true negatives in the predicted results). So accuracy is **(TN + TP) / (TN + TP + FN + FP)**\n",
    "\n",
    "In Scikit-learn, this is represented by the [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime accuracy isn't the best measure. For example, if you wanted to predict the presence of a disease that affected only 1 in every 1000 people, you can have a 99.9% accurate model just by predicting that nobody ever has the disease.\n",
    "\n",
    "In fact, we could easily create a model for the Titanic challenge that is 62% accurate by just predicting that everybody died!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Precision** and **recall**](https://en.wikipedia.org/wiki/Precision_and_recall) are less obvious terms, but depending on what is important in the problem, they may be better indicators of model performance.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/330px-Precisionrecall.svg.png)\n",
    "\n",
    "The **circle** represents the passengers that we **predicted to survive**.\n",
    "\n",
    "The **left hand side** represents the passengers that **actual survived**.\n",
    "\n",
    "**[Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)** is the proportion of passengers we **correctly predicted** to survive among all of the passengers we predicted to survive: **TP/(TP + FP)**\n",
    "\n",
    "**[Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)** is the proportion of passengers we correctly predicted to survive, among all of the passengers that actually survived: **TP/(TP + FN)**\n",
    "\n",
    "Examples when **high precision** is important (and we want to reduce the number of false positives):\n",
    "* **Trials**: of all of the people we declare/predict to be guilty, we want as many of these to be guilty as possible. Otherwise innocent people go to jail.\n",
    "* **Spam filter**: of all the emails we predict to be spam, we want as many to actually to be spam as possible (or else legitimate email gets flagged)\n",
    "\n",
    "Examples when **high recall** is important (and we want to reduce the number of false negatives):\n",
    "* **Cancer screening**: a false negative means somebody who got a negative result actually has cancer\n",
    "\n",
    "**Question**: what do you think is more important for youtube/netflix/spotify recommendation engines, precision or recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The scores for our model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Putting it all together ...\n",
    "\n",
    "We now have a lot of code spread out over a number of cells in this notebook.\n",
    "\n",
    "Here we combine it together so we can get a better picture of what the entire pipeline looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# More about this line shortly ...\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "\n",
    "# Choose features and lables\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(train_df[features], drop_first=True)\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Initialize model and fit to training data\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Use model to predict on unseen test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate how well the model did\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Repeatability ...\n",
    "\n",
    "Run the above cell a few times. What happens?\n",
    "\n",
    "A lot of the code related to machine learning depends on the output of random number generators, which can make repeating results difficult.\n",
    "\n",
    "Scikit-learn uses NumPy's random number generator, and luckily we can **seed** this random number generator (give the random generator in initial number so that the **same sequence of random numbers** are generated whenever that seed number is used).\n",
    "\n",
    "**Uncomment the line that looks like this: `np.random.seed(1337)`**\n",
    "\n",
    "Now run the code several times and watch the results.\n",
    "\n",
    "Note: the number `1337` isn't special (it's often used for seeds, as it's internet slang for 'leet' or 'elite'). \n",
    "\n",
    "Pick whichever number makes you happy -- but use it everytime for the same pipeline if you are interested in repeatability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## If a tree is good, is a forest better?\n",
    "\n",
    "What if we could set up a whole bunch of decision trees, each asking different questions, then vote on their final decisions to get a prediction? This is what Random Forest does.\n",
    "\n",
    "Random Forest is called an **ensemble** model, because it combines the results of multiple models to try to get a better answer.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In Scikit-learn, Random Forest is implemented by [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "Like with `DecisionTree`, we have a setting for the `max_depth` of the generated decision trees.\n",
    "\n",
    "We also have a setting for the number of trees to use, `n_estimators`.\n",
    "\n",
    "E.g., `model = RandomForestClassifier(n_estimators=100, max_depth=3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise: use a `RandomForestClassifier`\n",
    "\n",
    "Set up a machine learning pipeline using a `RandomForestClassifier` model:\n",
    "\n",
    "* Read the Titanic data from a file\n",
    "* Split the data into training and testing data\n",
    "* Fit a random forest model to our training data\n",
    "* Predict labels using our test data.\n",
    "* Evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Your code goes here ...\n",
    "# Hint: other than a couple of lines of code, it should look\n",
    "#   very much like the decision tree pipeline above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "show_solution('titanic-random-forest-pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Handling missing data\n",
    "\n",
    "Let's look at the information about our data set again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Suppose we thought that age was a good predictor of survival.\n",
    "\n",
    "We have a problem though:\n",
    "\n",
    "**Not every row has data for age recorded**\n",
    "\n",
    "Unfortunately, many machine learning algorithms **don't know how to handle empty data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some strategies to deal with this:\n",
    "\n",
    "* We could throw out any rows with missing data\n",
    "* We could substitute a default value (e.g., a mean if available, zero, -1, 9999, or some other value)\n",
    "* Maybe the presence of a null value could be a feature in itself? (e.g., record a one if there was a cabin number recorded for the passenger, and a zero otherwise)\n",
    "\n",
    "Let's look at how to throw out rows, and how to replace a null value with a mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Throwing out rows\n",
    "\n",
    "First, here is a tool for detecting the presence of missing (null) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing data in each column\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes have a method called [`dropna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) that can be used to filter out rows that have missing data.\n",
    "\n",
    "We can specify which columns to look at using the `subset` keyword argument.\n",
    "\n",
    "Note that this method by default does not modify the dataframe in place, but **returns a new dataframe** with the rows missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_non_null_train_df = train_df.dropna(subset=['Age'])\n",
    "age_non_null_train_df.info()\n",
    "\n",
    "# If we decide to go further down this road, we might do either:\n",
    "#    train_df.dropna(subset=['Age'], inplace=True)\n",
    "#                or\n",
    "#    train_df = train_df.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Replacing missing data with a mean\n",
    "\n",
    "With this strategy, we don't throw out any rows, but instead create a 'fictional' age value for the rows with missing age.\n",
    "\n",
    "Let's look at the mean age from the non-null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `Series` have a method called [`fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html) that can replace null values in a column with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a copy of the dataframe if we don't want to\n",
    "# modify the original (optional)...\n",
    "age_mean_train_df = train_df.copy()\n",
    "# Overwrite the column with new data with the missing data filled\n",
    "age_mean_train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: if you wanted to get a sense of the distribution of ages in the data, you could run value counts using binning with the `bins` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Age\"].value_counts(bins=10, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the age of the passenger as an additional feature in a machine learning pipeline.\n",
    "\n",
    "* Use either the dataset with the null rows thrown out (e.g., `train_df = age_non_null_train_df`) or the dataset with the missing age data replaced by mean (e.g., `train_df = age_mean_train_df`). You decide.\n",
    "* Use which ever classifier algorithm you'd like (`DecisionTreeClassifier` or `RandomTreeClassifier`).\n",
    "* Play with whichever keyword arguments you might like to change ... any effect on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "# (one possible solution ...)\n",
    "show_solution('titanic-age-dropna.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Predict your own survival (or possibly that of your entire family) with the most recently trained model.\n",
    "\n",
    "E.g.,\n",
    "```\n",
    "# You may need to sanity check the order of features, should look like:\n",
    "# [\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Sex_male\"]\n",
    "print(X_train.columns)\n",
    "features = X_train.columns\n",
    "\n",
    "family = [\n",
    "  [2, 1, 1, 53.0, 1], # Me\n",
    "  [2, 1, 1, 52.0, 0], # Wife\n",
    "  [2, 0, 2, 10.0, 0]  # Daughter\n",
    "]\n",
    "\n",
    "family_df = pd.DataFrame(family, columns=features)\n",
    "model.predict(family_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "One potential problem to look out for in machine learning is overfitting. This occurs when your model is so finely tuned to the data you give it, that it only works well with that data, and does a poor job when it encounters new unseen data. This can happen when too many adjustable parameters to a model are used than what is optimal. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/200px-Overfitting.svg.png)\n",
    "\n",
    "From Wikipedia: `The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data, compared to the black line.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Onto the next notebook, on [regression](02-regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
