{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-slovakia",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Unlike with **classification** where we are predicting a **nominal** label, in **regression** problems we are predicting a **numerical** quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-zimbabwe",
   "metadata": {},
   "source": [
    "The simplest form of regression is **linear regression** (also known as the **least squares method**). This is a problem from calculus that tries to find a line that minimizes the squared distance from a collection of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-multimedia",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/200px-Linear_regression.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-server",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Linear_least_squares_example2.png/190px-Linear_least_squares_example2.png)\n",
    "\n",
    "From Wikipedia: `In linear regression, the observations (red) are assumed to be the result of random deviations (green) from an underlying relationship (blue) between a dependent variable (y) and an independent variable (x).`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-attachment",
   "metadata": {},
   "source": [
    "The method also handles fitting lines in multidimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our prerequisites\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download solution(s)\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_data(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    if not os.path.exists('solutions'):\n",
    "        os.mkdir('solutions')\n",
    "    url = 'https://raw.githubusercontent.com/ualberta-rcg/python-machine-learning/main/notebooks/' + path\n",
    "    output_file = path\n",
    "    urllib.request.urlretrieve(url, output_file)\n",
    "    print(\"Downloaded \" + path)\n",
    "\n",
    "def show_solution(file):\n",
    "    fp = open('solutions/{}'.format(file), 'r')\n",
    "    print(fp.read())\n",
    "\n",
    "download_data('solutions/real-estate-san-fran.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-endorsement",
   "metadata": {},
   "source": [
    "## A problem: California Housing Prices\n",
    "\n",
    "Scikit-learn comes with a number of datasets (or will fetch the data) that are good for exploring techiniques in data science.\n",
    "\n",
    "The dataset we will fetch relates to California Housing prices from the 1990 census, where the data is organized in blocks groups (geographically close, between 600 - 3000 houses in each block).\n",
    "\n",
    "Our task will be to predict house prices (the label or target) given other features in the data.\n",
    "\n",
    "We fetch the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "california = sklearn.datasets.fetch_california_housing()\n",
    "california"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-disability",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The data downloaded is a dictionary, which includes the housing data and some metadata. We can take a look at the description that comes with this download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(california['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "california.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-arcade",
   "metadata": {},
   "source": [
    "We can assemble a dataframe from the various components downloaded. The downloaded data is already separated into feature and labels, so we will combine both into our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df = pd.DataFrame(california['data'], columns=california['feature_names'])\n",
    "california_df['MedHouseVal'] = california['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-bandwidth",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "This is a neat scatter plot of the data, mapping the longiture/latitude, making the size relative to the population of the blocks, and coloring based on the median house values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\",\n",
    "                   alpha=0.4,\n",
    "                   s=california_df[\"Population\"]/100,\n",
    "                   label=\"Population\",\n",
    "                   c=\"MedHouseVal\", \n",
    "                   figsize=(12,8),\n",
    "                   cmap=plt.get_cmap(\"jet\"),\n",
    "                   colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-acrobat",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can visualize how some of the variables vary with each other using the Pandas function [`scatter_matrix`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html).\n",
    "\n",
    "Here we will look at how median housing value varies with:\n",
    "* median income\n",
    "* average number of rooms\n",
    "* house age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"MedHouseVal\", \"MedInc\", \"AveRooms\", \"HouseAge\"]\n",
    "scatter_matrix(california_df[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-survivor",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "\n",
    "Looking at the above, we some very sparce data values at the high ends of some of these graphs.\n",
    "\n",
    "Lets look a little further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-ensemble",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "There are no correct answers when deciding how to deal with outliers.\n",
    "\n",
    "We'll decide to throw out any data that is **greater than three standard deviations** away from the mean for a variable.\n",
    "\n",
    "Lets look at the standard deviation of median income:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.MedInc.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-tunnel",
   "metadata": {},
   "source": [
    "The upper cut off value for median income (above which the data is declared to be an outlier) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_cut_off = california_df.MedInc.mean() + 3 * california_df.MedInc.std()\n",
    "income_cut_off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-deputy",
   "metadata": {},
   "source": [
    "The rows in the dataset that fall within the acceptable range are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.MedInc < income_cut_off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-plumbing",
   "metadata": {},
   "source": [
    "We can now replace our dataset with one that only includes these rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df = california_df[california_df.MedInc < income_cut_off]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-intellectual",
   "metadata": {},
   "source": [
    "Looking at the dataset again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-crest",
   "metadata": {},
   "source": [
    "## Exercise: remove the outliers from the average rooms data\n",
    "\n",
    "Use three standard deviations from the mean as your guide. (See the **Putting it all together** section below for a possible solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-ordinary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can look at the descriptive statistics and the `scatter_matrix` again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(california_df[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-moses",
   "metadata": {},
   "source": [
    "## Selecting features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = california_df[[\"MedInc\", \"AveRooms\", \"HouseAge\"]]\n",
    "y = california_df[\"MedHouseVal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-southwest",
   "metadata": {},
   "source": [
    "## Spliting the data into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-joining",
   "metadata": {},
   "source": [
    "## Selecting a model and training on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-finance",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Again, we use the `predict` method to make predictions on some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the first few predictions\n",
    "\n",
    "print(predictions[:10])\n",
    "print(list(y_test.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-trouble",
   "metadata": {},
   "source": [
    "## Evaluating regression models\n",
    "\n",
    "There are some measures we can use for evaluating the quality of our predictions\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "$MAE = (\\sum_N |actual_i - predicted_i|) / N$\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$MSE = \\sum_N (actual_i - predicted_i)^2 / N$\n",
    "\n",
    "### Coefficient of Determination ($R^2$)\n",
    "\n",
    "$R^2 = $ (math goes here)\n",
    "\n",
    "* 1 is a perfect score, when all predictions are correct (interpretation: the model explains all the variability of the labels around their mean)\n",
    "* If your model only every predicted the mean of the labels, you would get a score of zero (interpretation: the model explains none of the variability of the labels around their mean)\n",
    "* Can be less than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, predictions))\n",
    "print('R2 Score:', r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-rendering",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "What do you think of this model? Did we explain all of the variability in housing prices?\n",
    "\n",
    "Unfortunately, pricing houses can be complicated ...\n",
    "\n",
    "![](https://i.insider.com/4fb69a54eab8ea195000000d?width=750&format=jpeg&auto=webp)\n",
    "\n",
    "---\n",
    "\n",
    "Question: Are there any other features (either included with the data, or engineered) you might use to help improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-combat",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "The code is pretty scattered in this notebook. Lets look at the pipeline in it's entirety:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set a random seed for reproducability\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Get/load the data into a dataframe ...\n",
    "california = sklearn.datasets.fetch_california_housing()\n",
    "california_df = pd.DataFrame(california['data'], columns=california['feature_names'])\n",
    "california_df['MedHouseVal'] = california['target']\n",
    "\n",
    "# Remove outliers ...\n",
    "income_cut_off = california_df.MedInc.mean() + 3 * california_df.MedInc.std()\n",
    "california_df = california_df[california_df.MedInc < income_cut_off]\n",
    "\n",
    "room_cut_off = california_df.AveRooms.mean() + 3 * california_df.AveRooms.std()\n",
    "california_df = california_df[california_df.AveRooms < room_cut_off]\n",
    "\n",
    "# Choose features/labels\n",
    "X = california_df[[\"MedInc\", \"AveRooms\", \"HouseAge\"]]\n",
    "y = california_df[\"MedHouseVal\"]\n",
    "\n",
    "# Split data into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Select model and train it\n",
    "model = LinearRegression()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate predictions\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, predictions))\n",
    "print('R2 Score:', r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fe76b",
   "metadata": {},
   "source": [
    "## Retrieving the parameters of the linear model\n",
    "\n",
    "The linear coeffients (i.e., the gradient or slope) are in the `model.coef_` attribute of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e81c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b018685",
   "metadata": {},
   "source": [
    "The interecept is in `model.intercept_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c24395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150561e",
   "metadata": {},
   "source": [
    "We can calculate the models prediction by hand for the first row of data by taking the dot product of the coefficients and adding the intercept ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test.iloc[0].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dot(model.coef_) + model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730719a2",
   "metadata": {},
   "source": [
    "Comparing this with the output from `model.predict` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efca707",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-greek",
   "metadata": {},
   "source": [
    "## Other linear regression models\n",
    "\n",
    "Scikit-learn provides a number of other [variations](\n",
    "https://scikit-learn.org/stable/modules/linear_model.html) on linear regression you can try for fitting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-truth",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Decision Tree Classifiers can be combined into the ensemble model Random Forest. Similarly, we can also use ensemble models for regression tasks. One class that impliments this in Scikit-learn is [`sklearn.ensemble.GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html).\n",
    "\n",
    "The idea behind this algorithm is a bit complicated, with a lot of low-level details. I asked some of my Compute Canada colleagues what they thought of this following desciption, intended for a novice audience:\n",
    "\n",
    "> We create an accurate model (a **\"strong learner\"**) by combining the inputs of several less accurate models (or **\"weak learners\"**). In Scikit-learn, these weak learners are instances of `DecisionTreeRegressor`. Using a step-by-step approach, the weak learners are added to the ensemble, and the weights that determine each weak learner's contribution are tweaked via an optimization algorithm.\n",
    "\n",
    "My colleague Mat suggested the following additional information:\n",
    "\n",
    "> The other way you could go about it is that we have 5 kids, johnny is good at x, suzie is good at y, and so on, and we defer to the kid who knows the most about a specific problem.\n",
    ">\n",
    "> So each kid gets to put in an input based on how well they know that sort of problem\n",
    ">\n",
    "> I would lean into anthropomorphizing the problem, and treating the model like a team of people who don't necessarily have the whole answer themselves, but working together, they perform much better. You can talk about the weighting algorithm as the \"supervisor\" or \"manager\" of the team\n",
    "\n",
    "My colleague Lucas also added:\n",
    "\n",
    "> I assume they're familiar with the concept of error when you get to talking about grad boosting? If so I'd say something like: Train a simple decision tree. That tree will make a certain number of prediction errors. Take those errors and train another tree on them. Add the \"correction\" to the original tree. This new \"corrected\" tree will also make some prediction errors. Take those and train another tree... keep doing that, \"compensating\" for the errors made by your tree with a subsequent tree, until convergence or you're satisfied with a (hopefully low) error rate\n",
    "\n",
    "The nice thing is that you don't need to know too much about the low level details to implement a pipeline using this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-drill",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement a regression pipeline using the `GradientBoostingRegressor`.\n",
    "\n",
    "Look at the linked documentation for the default values of `n_estimators`, `max_depth`, and `learning_rate`. Feel free to modify the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-copper",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "In our real estate example, engineer a feature that represents the distance from San Fransisco, and set up a regression pipeline. Can you think of any other similar features that might help boost the performance of our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-ensemble",
   "metadata": {},
   "source": [
    "To see one possible solution ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "show_solution('real-estate-san-fran.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-nightmare",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "On to the next notebook, on [unsupervised learning](03-unsupervised.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
