{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Learn the jargon and understand the pipelines of Machine Learning\n",
    "* Set up a classification model\n",
    "* Set up a regression model\n",
    "* Set up an unsupervised (clustering) model\n",
    "---\n",
    "\n",
    "Approach to learning: I would like to take an approach where we are playing with data as early as possible. As such, we will be talking about concepts as we encounter them in the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "\n",
    "Loosly, machine learning can be defined as:\n",
    "\n",
    "```Algorithms that allow a computer to predict patterns in unseen data based on learning done on data that has previously been seen```\n",
    "\n",
    "Typically, the more data you allow the computer to learn with, the better job it will do.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use machine learning?\n",
    "\n",
    "* Fraud detection\n",
    "* Spam detection\n",
    "* Credit risk\n",
    "* Voice recognition\n",
    "* Image recognition\n",
    "* Recommendations (search engines)\n",
    "* Finding patterns in the stock market\n",
    "* Housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional modeling vs machine learning\n",
    "\n",
    "Usually in science we work with **white box model**, like a set of equations, and we fully understand how our models work.\n",
    "\n",
    "[Image of equations]\n",
    "\n",
    "With machine learning, we let the machine figure out the details. While we understand the algorithms that allow the computer to learn, we don't always understand the insights or details about the specific insights the algorithms learn about the data (**black box model**).\n",
    "\n",
    "[Image of black box with inputs outputs]\n",
    "\n",
    "This feature is sometimes used as an argument against the practice of machine learning.\n",
    "\n",
    "Here is a thought provoking [blog entry about this from Rich Sutton](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) (pioneer of reinforcement learning, U of A faculty).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process\n",
    "\n",
    "\n",
    "* **Define the problem**\n",
    " * This can be difficult. What are we trying to achieve? What are we trying to predict?\n",
    "* **Get the data**\n",
    " * This is often connected with the problem definition step, because knowing about the data helps clarify what we can do with it\n",
    "* **Prepare data** \n",
    " * Exploratory data analysis and visualization\n",
    " * Cleaning data\n",
    " * Often the most tedious and time consuming step\n",
    "* **Select Algorithm**\n",
    " * Setting up one or more machine learning pipelines \n",
    "* **Train the model**\n",
    " * Feed the algorithm data.\n",
    "* **Test the mode**\n",
    "  * Maybe we need to go back and select a different algorithm to work with?\n",
    "* **Select the best model** \n",
    " * The definition of \"best\" depends on the type of problem, the type of data, and our goals\n",
    "* **Predict**\n",
    "  * Use the model to make predictions based on unseen data\n",
    "* $$$ (?)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data science tools in Python we will be using\n",
    "\n",
    "* Data analysis and cleaning/transforming: **pandas**\n",
    "* Visualization: matplotlib (possibly **seaborn** and **plotly**)\n",
    "* Scientific computing/number crunching: **numpy**\n",
    "* Machine learning algorithms: **Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make sure that we have the tools available to us ...\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... if not, may need to uncomment one or more of:\n",
    "# (Replace pip with conda where applicable)\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data for this notebook\n",
    "\n",
    "We can download the data if we don't have it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and solutions\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download_data(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    if not os.path.exists('solutions'):\n",
    "        os.mkdir('solutions')\n",
    "    url = 'https://raw.githubusercontent.com/ualberta-rcg/python-machine-learning/master/notebooks/' + path\n",
    "    output_file = path\n",
    "    urllib.request.urlretrieve(url, output_file)\n",
    "    print(\"Downloaded \" + path)\n",
    "\n",
    "download_data('data/titanic/train.csv')\n",
    "download_data('data/titanic/test.csv')\n",
    "download_data('solutions/titanic-passenger-class.py')\n",
    "download_data('solutions/titanic-random-forest-pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Machine Learning tools\n",
    "\n",
    "Four packages that are available to us for free:\n",
    "\n",
    "* Scikit-learn\n",
    " * Easy to understand\n",
    " * Great for learning\n",
    " * Consistent interface\n",
    "* Tensorflow\n",
    " * From Google\n",
    " * Takes advantage of GPUs\n",
    "* Pytorch\n",
    " * From Facebook\n",
    " * Takes advantage of GPUs\n",
    "* Keras\n",
    " * Built on top of Tensorflow\n",
    " * Easier to understand and use\n",
    "\n",
    "Some comparisons:\n",
    "https://towardsdatascience.com/scikit-learn-tensorflow-pytorch-keras-but-where-to-begin-9b499e2547d0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A problem: The Titanic Kaggle challenge\n",
    "\n",
    "We will learn some of the jargon of machine learning by walking through an example.\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/) (owned by Google) is a machine learning competition website.\n",
    "\n",
    "Competitions are either for fun, for money, or might lead to a job offer.\n",
    "\n",
    "The introductory competition involves the sinking of the [Titanic](https://www.kaggle.com/c/titanic):\n",
    "\n",
    "```\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n",
    "```\n",
    "\n",
    "In the competition, we are given 891 rows of data (one row per passenger) in the file `train.csv` where we know whether the passenger survived. As the name indicates, we will use this data to train a machine learning model.\n",
    "\n",
    "There is also a file called `test.csv` that includes information on 418 other passengers, but missing from this data is whether the passenger survived or not. To enter the competition, we make predictions on this file and submitting it to the competition.\n",
    "\n",
    "I've done two submissions, one which guessed 71% correctly, the other guessed 76% correctly.\n",
    "\n",
    "We'll explore this problem to learn some of the concepts related to machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An initial look ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the first 10 records ...\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any good candidates for predictors of survival?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "\n",
    "In short, inputs to a machine learning model are called **features** (or **attribute**), and the output predicted is called a **label**.\n",
    "\n",
    "The **label** that we are trying to predict in the Titanic challenge is clearly the **Survived** data.\n",
    "\n",
    "We have 11 potential **features** we can use to predict this **label**.\n",
    "\n",
    "* A feature can be binary, nominal, or numerical.\n",
    "* We want to choose features that have predictive power.\n",
    "* We want to choose features that are as independent as possible\n",
    "    * E.g., if `weight_in_pounds` is a predictive feature in a model, don't also choose `weight_in_kilograms`\n",
    "* Note that we can also design our own features from the data provided. (E.g., if we are predicting stock prices and we are given the previous opening and closing prices of a stock, maybe the difference would be a good predictor?). The process of designing and choosing features is called **feature engineering**.\n",
    "\n",
    "A single variable in the data set that is \n",
    "* Feature/Attribute\n",
    "  * A single variable (binary, nominal, numerical)\n",
    "\n",
    "\n",
    "* Label/Class\n",
    " * An extra information that categorizes/classifies a given instance\n",
    "\n",
    "* Instance/Feature vector\n",
    "  * One entity described by features\n",
    "* Dataset\n",
    " * Collection of labeled or unlabeled instances\n",
    "---\n",
    "\n",
    "**Question**: what could be possible features for a model that predicts housing prices?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Learning\n",
    "\n",
    "### Supervised\n",
    "* All instances in training data are **labeled**\n",
    "* **Classification** - predicting nominal label\n",
    "  * We are looking to build models that separate data into distinct classes\n",
    "  * Algorithms:\n",
    "    * Decision Trees\n",
    "    * Random Forest\n",
    "    * Support Vector Machine\n",
    "  * E.g.,\n",
    "    * Did the person survive the titanic? (True/False)\n",
    "    * What species of plant is this?\n",
    "* **Regression** - predicting numerical label\n",
    "  * Based on previous data, predict a continuous numerical quantity\n",
    "  * Algorithms\n",
    "    * Linear regression\n",
    "    * Polynomial regression\n",
    "  * E.g.,\n",
    "    * Predict the high temperature for tomorrow\n",
    "    * Predict the closing price of a stock tomorrow\n",
    "\n",
    "### Unsupervised\n",
    "* There are **no labels** for the instances\n",
    "* We are trying to find hidden meaning in data without additional guidance\n",
    "* E.g., Find ten categories that a collection of emails fall into (clustering)\n",
    "* Algorithms:\n",
    "  * KMeans\n",
    "  * KNN\n",
    "\n",
    "### Reinforcement Learning\n",
    "* Algorithms learn how to make actions on data points based on environment responses\n",
    "* It’s impossible to get label without making an action\n",
    "* Check this out, a DeepMind model learning to play Atari Breakout: https://www.youtube.com/watch?v=V1eYniJ0Rnk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is poking around data (e.g., looking at descriptive statistics or plots) to gather insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The `value_counts` method for a series is useful for exploring the data. It in essense gives us the frequency table from a column in the data.\n",
    "\n",
    "Lets find out the survival rate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_df['Survived'].value_counts()\n",
    "percent = round(100 * counts[1] / len(train_df))\n",
    "print('Survival rate: {}%'.format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the influence of various features on survival rate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_counts = train_df['Sex'].value_counts()\n",
    "sex_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how the series returned from `value_counts` is organized\n",
    "\n",
    "print(sex_counts.keys())\n",
    "print(sex_counts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data for the males alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df = train_df[train_df['Sex'] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now the survival rate for males ...\n",
    "\n",
    "counts = male_df['Survived'].value_counts()\n",
    "percent = round(100 * counts[1] / len(male_df), 2)\n",
    "print('Survival rate: {}%'.format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you will have recognized that you have done an almost identical set of operations on two different dataframes ... this sounds like a good time to create a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_report(df):\n",
    "    if len(df) == 0:\n",
    "        print('Empty data')\n",
    "        return\n",
    "    counts = df['Survived'].value_counts()\n",
    "    print('             Lived: {}'.format(counts[1]))\n",
    "    print('              Died: {}'.format(counts[0]))\n",
    "    print('Chance of survival: {}%'.format(round(100 * counts[1] / len(df), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_report(male_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new function makes exploring the data for the female passengers easy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_report(train_df[train_df['Sex'] == 'female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: is gender a good predictor of survival?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Using our `survival_report` function above, explore the survival rates for the values of Passenger class ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "!cat solutions/titanic-passenger-class.py\n",
    "\n",
    "## Load solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Sex', 'Pclass']\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "\n",
    "# feat_column_names = list(tdf.columns[2:])\n",
    "#features.remove('Survived')\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(train_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['Survived']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "The model we are going to initially try is a decision tree.\n",
    "\n",
    "![](https://www.seriouseats.com/images/20100120-flowchart-floorfood.jpg)\n",
    "\n",
    "A number of questions are asked about the data, and decisions are made based on the answers. These answers are refined or changed based on additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "The [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) looks at your data and figures out what questions to ask automatically.\n",
    "\n",
    "We can specify an option for `max_depth` (basically, how deep do we want the questions to go)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `fit`\n",
    "\n",
    "Most models (all?) from Scikit-learn have a `fit` method that trains the model based on your features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit doesn't modify the model in place\n",
    "# (returns a trained model)\n",
    "\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `predict`\n",
    "\n",
    "Most models (all?) from Scikit-learn have a `predict` method that allows a trained model to make predictions when given unlabeled features.\n",
    "\n",
    "Here we use our **test data** (data that the model wasn't trained on) as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first ten predicted labels for the test data\")\n",
    "print(list(predictions[:10]))\n",
    "print('The ten actual labels')\n",
    "print(list(y_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what did the decision tree do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to install graphviz ....\n",
    "# Note for conda, you may have to install both graphviz and python-graphviz\n",
    "# !pip install graphviz\n",
    "# !conda install graphviz python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "dot_data = export_graphviz(model,\n",
    "                           feature_names=X.columns,\n",
    "                           class_names=['Died', 'Survived'],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True,\n",
    "                           out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the quality of our predictions\n",
    "\n",
    "Ah, statistics and jargon!\n",
    "\n",
    "**Accuracy** is the proportion of predictions that are correct.\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "**Precision** and **recall** are less obvious terms, and tied to **Type 1** and **Type 2** errors in statistics.\n",
    "\n",
    "**Type I error** is the rejection of a true null hypothesis (also known as a **\"false positive\"**)\n",
    "* E.g., an innocent person is convicted\n",
    "* E.g., a healthy person got a medical test saying they are sick\n",
    "* E.g., a legitimate email is marked as spam\n",
    "* E.g., a predicted Titanic survivor is actually dead\n",
    "\n",
    "**Type II error** is the non-rejection of a false null hypothesis (also known as a **\"false negative\"**)\n",
    "* E.g., a guilty person is not convicted\n",
    "* E.g., a sick person got a medical test saying they are healthy\n",
    "* E.g., a legitimate spam is not marked as spam\n",
    "* E.g., a predicted dead person actual survived the Titanic\n",
    "\n",
    "**Sensitivity**: is the \"True Positive rate\", it measures the proportion of positives that are correctly identified\n",
    "* E.g., proportion of guilty people correctly convicted\n",
    "* E.g., proportion of sick people the medical test correctly identifies as sick\n",
    "* E.g., proportion of spam that is correctly identified as spam\n",
    "* E.g., proportion of Titanic survivors identified as survivors\n",
    "\n",
    "**Specificity** is the \"True Negative rate\", it measures the proportion of negatives that are correctly identified\n",
    "* E.g., proportion of innocent people not convicted\n",
    "* E.g., proportion of healthy people the medical test correctly identify as healthy\n",
    "* E.g., proportion of legitimate email that goes to our inbox (not marked as spam)\n",
    "* E.g., proportion of dead Titanic passengers predicted to be dead\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, predictions)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = matrix[0][0] # True negatives\n",
    "fn = matrix[1][0] # False negatives\n",
    "fp = matrix[0][1] # False positives\n",
    "tp = matrix[1][1] # True positives\n",
    "print('True negatives: {}'.format(tn))\n",
    "print('False negatives: {}'.format(fn))\n",
    "print('False positives: {}'.format(fp))\n",
    "print('True positives: {}'.format(tn))\n",
    "print('Sensitivity: {}'.format(tp / (tp + fn)))\n",
    "print('Specifity: {}'.format(tn / (tn + fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Putting it all together ...\n",
    "\n",
    "We now have a lot of code spread out over a number of cells in this notebook.\n",
    "\n",
    "Here we combine it together so we can get a better picture of what the entire pipeline looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# More about this line shortly ...\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "\n",
    "# Choose features and lables\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(train_df[features])\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Initialize model and fit to training data\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Use model to predict on unseen test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate how well the model did\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Repeatability ...\n",
    "\n",
    "Run the above cell a few times. What happens?\n",
    "\n",
    "A lot of the code related to machine learning depends on the output of random number generators, which can make repeating results difficult.\n",
    "\n",
    "Scikit-learn uses NumPy's random number generator, and luckily we can **seed** this random number generator (give the random generator in initial number so that the **same sequence of random numbers** are generated whenever that seed number is used).\n",
    "\n",
    "**Uncomment the line that looks like this: `np.random.seed(1337)`**\n",
    "\n",
    "Now run the code several times and watch the results.\n",
    "\n",
    "Note: the number `1337` isn't special (it's often used for seeds, as it's internet slang for 'leet' or 'elite'). \n",
    "\n",
    "Pick whichever number makes you happy -- but use it everytime for the same pipeline if you are interested in repeatability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## If a tree is good, is a forest better?\n",
    "\n",
    "What if we could set up a whole bunch of decision trees, each asking different questions, then vote on their final decisions to get a prediction? This is what Random Forest does.\n",
    "\n",
    "Random Forest is called an **ensemble** model, because it combines the results of multiple models to try to get a better answer.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In Scikit-learn, Random Forest is implemented by [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "Like with `DecisionTree`, we have a setting for the `max_depth` of the generated decision trees.\n",
    "\n",
    "We also have a setting for the number of trees to use, `n_estimators`.\n",
    "\n",
    "E.g., `model = RandomForestClassifier(n_estimators=100, max_depth=3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise: use a `RandomForestClassifier`\n",
    "\n",
    "Set up a machine learning pipeline using a `RandomForestClassifier` model:\n",
    "\n",
    "* Read the Titanic data from a file\n",
    "* Split the data into training and testing data\n",
    "* Fit a random forest model to our training data\n",
    "* Predict labels using our test data.\n",
    "* Evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Your code goes here ...\n",
    "# Hint: other than a couple of lines of code, it should look\n",
    "#   very much like the decision tree pipeline above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "!cat solutions/titanic-random-forest-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Handling missing data\n",
    "\n",
    "Let's look at the information about our data set again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Suppose we thought that age was a good predictor of survival.\n",
    "\n",
    "We have a problem though:\n",
    "\n",
    "**Not every row has data for age recorded**\n",
    "\n",
    "Unfortunately, many machine learning algorithms **don't know how to handle empty data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some strategies to deal with this:\n",
    "\n",
    "* We could throw out any rows with missing data\n",
    "* We could substitute a default value (e.g., a mean if available, zero, -1, 9999, or some other value)\n",
    "\n",
    "Let's look at how to throw out rows, and how to replace a null value with a mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Throwing out rows\n",
    "\n",
    "First, here is a tool for detecting the presence of missing (null) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing data in each column\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes have a method called [`dropna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) that can be used to filter out rows that have missing data.\n",
    "\n",
    "We can specify which columns to look at using the `subset` keyword argument.\n",
    "\n",
    "Note that this method by default does not modify the dataframe in place, but **returns a new dataframe** with the rows missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_non_null_train_df = train_df.dropna(subset=['Age'])\n",
    "age_non_null_train_df.info()\n",
    "\n",
    "# If we decide to go further down this road, we might do either:\n",
    "#    train_df.dropna(subset=['Age'], inplace=True)\n",
    "#                or\n",
    "#    train_df = train_df.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Replacing missing data with a mean\n",
    "\n",
    "With this strategy, we don't throw out any rows, but instead create a 'fictional' age value for the rows with missing age.\n",
    "\n",
    "Let's look at the mean age from the non-null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `Series` have a method called [`fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html) that can replace null values in a column with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a copy of the dataframe if we don't want to\n",
    "# modify the original (optional)...\n",
    "age_mean_train_df = train_df.copy()\n",
    "# Overwrite the column with new data with the missing data filled\n",
    "age_mean_train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use age as an additional feature in a machine learning pipeline.\n",
    "\n",
    "* Use either the dataset with the null rows thrown out (e.g., `train_df = age_non_null_train_df`) or the dataset with the missing age data replaced by mean (e.g., `train_df = age_mean_train_df`). You decide.\n",
    "* Use which ever classifier algorithm you'd like (`DecisionTreeClassifier` or `RandomTreeClassifier`).\n",
    "* Play with whichever keyword arguments you might like to change ... any effect on the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem**: we have some data, and we aren't given a label that neatly categorizes it. But we want to separate the data in some meaningful way (in clusters, using some measure of nearness).\n",
    "\n",
    "**We need to supply how many clusters we want the algorithm to find ahead of time.**\n",
    "\n",
    "We don't know what the clusters represent, just that we are hoping that there will be a division in the data that will help us understand it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we'll explore unsupervised clustering with an algorithm called KMeans.\n",
    "\n",
    "First, lets create a function to create a mock dataset for us.\n",
    "\n",
    "The function will sample one thousand points in the x-y plane (`blobs`) from 3 different probability distributions using the Scikit-learn function `make_blobs`. We will keep track of which distribution each point is sampled from (`cluster_labels`).\n",
    "\n",
    "These will be returned from our function and stored in the variables `xy_points` and `labels` (**note: the KMeans algorithm won't know about the label here, but we can use it in this contrived example to examine the output**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "numpy.random.seed(1337)\n",
    "\n",
    "centers = [[-10, -10], [-10, 13], [8, -1]]\n",
    "\n",
    "def get_points_and_labels(**kwargs):\n",
    "    blobs, cluster_labels = make_blobs(n_samples=1000, n_features=2,\n",
    "                                       centers=centers, cluster_std=5.0)\n",
    "    return blobs, cluster_labels\n",
    "\n",
    "xy_points, labels = get_points_and_labels(initialize_seed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets make a function that will visualize our x-y points, optionally coloring the points if the labels are also included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(title, xy_points, labels=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    xy_points_df = pd.DataFrame(xy_points, columns=['x', 'y'])\n",
    "\n",
    "    if labels is None:\n",
    "        plt.scatter(xy_points_df.x, xy_points_df.y, c=\"grey\")\n",
    "    else:\n",
    "        xy_points_df['labels'] = pandas.Series(labels)\n",
    "        colours = [\"red\", \"blue\", \"green\"]\n",
    "        clusters = [0, 1, 2]\n",
    "        for cluster_id in clusters:\n",
    "            cluster_data = \\\n",
    "                xy_points_df.loc[xy_points_df[\"labels\"] == cluster_id,\n",
    "                                 [\"x\", \"y\"]]\n",
    "            plt.scatter(cluster_data.x, cluster_data.y,\n",
    "                        c=colours[cluster_id-1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our x-y points both with and without the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('First 10 xy_points: \\n',xy_points[:10])\n",
    "plot_clusters('Unlabeled clusters', xy_points)\n",
    "print('First 10 labels: \\n', labels[:10])\n",
    "plot_clusters('Labeled clusters', xy_points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering) ...\n",
    "\n",
    "---\n",
    "\n",
    "1. k initial randomly chosen \"means\" (or \"seeds\", in this case k=3) are randomly generated within the data domain (shown in color).\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/200px-K_Means_Example_Step_1.svg.png)\n",
    "\n",
    "---\n",
    "\n",
    "2. k clusters are created by associating every observation with the nearest mean.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/200px-K_Means_Example_Step_2.svg.png)\n",
    "\n",
    "---\n",
    "\n",
    "3. The centroid of each of the k clusters becomes the new mean.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/200px-K_Means_Example_Step_3.svg.png)\n",
    "\n",
    "---\n",
    "\n",
    "4. Steps 2 and 3 are repeated until convergence has been reached (not quaranteed)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/200px-K_Means_Example_Step_4.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In Scikit-learn, KMeans is provided by the [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class.\n",
    "\n",
    "Notice that we specify the number of clusters we want the algorithm to find `n_clusters`.\n",
    "The setting `n_init=1000` means that we will try 1000 times with different initial means, then choose the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=1000)\n",
    "kmeans.fit(xy_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nActual cluster means')\n",
    "for x_y in centers:\n",
    "    print('%f,%f' % (x_y[0], x_y[1]))\n",
    "    \n",
    "print('\\nPredicted cluster means')\n",
    "for x_y in kmeans.cluster_centers_:\n",
    "    print('%f,%f' % (x_y[0], x_y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans.predict(xy_points)\n",
    "\n",
    "print('First 10 actual labels: ', labels[:10])\n",
    "print('First 10 computed labels: ', kmeans_labels[:10])\n",
    "\n",
    "plot_clusters('Re-plot of original clusters', xy_points, labels)\n",
    "plot_clusters('Calculated clusters', xy_points, kmeans_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huh?\n",
    "\n",
    "Notice that most of the predicted labels are actually wrong!\n",
    "\n",
    "KMeans finds clusters, but it has no way of knowing what the actual labels mean. It just detects clusters.\n",
    "\n",
    "You will notice in the above plot that the shape of the clusters are pretty close, but the colors of the individual clusters might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_points2, labels2 = get_points_and_labels()\n",
    "kmeans_labels2 = kmeans.predict(xy_points2)\n",
    "\n",
    "plot_clusters('New clusters', xy_points2, labels2)\n",
    "plot_clusters('New predicted clusters', xy_points2, kmeans_labels2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
